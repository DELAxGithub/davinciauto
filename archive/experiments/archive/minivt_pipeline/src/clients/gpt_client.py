import os, json, subprocess, tempfile, shlex
from typing import List, Optional
from pathlib import Path

# Optional OpenAI SDK import (only used when OPENAI_API_KEY is set)
try:
    import openai  # type: ignore
except Exception:
    openai = None
from utils.llm_cost import LLMCostTracker

class GPTClient:
    def __init__(self, model: str = "gpt-4o-mini", demo_mode: bool = False):
        self.model = model
        # Allow env override to force demo mode
        env_demo = os.getenv("GPT_DEMO_MODE")
        self.demo_mode = True if env_demo == "1" else demo_mode
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.llm_cost = LLMCostTracker()
        self.last_cost: Optional[dict] = None
        # External CLI hook
        self.cli_cmd = os.getenv("LLM_CLI_CMD")  # e.g., "llm -m gemini-pro --file {file}" or "claude --file --format json"
        # Gemini API direct
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        # Force latest default: Gemini 2.5 Pro (can be overridden by env)
        self.gemini_model = "gemini-2.5-pro"
        os.environ["GEMINI_MODEL"] = self.gemini_model
        # Preferred provider: default to env or gemini (GUI may override)
        self.preferred_provider = os.getenv("LLM_PROVIDER", "gemini")
        
        # Claude Codeåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
        self.claude_code_available = self._check_claude_code()
        
        # Claude Codeæ¡ˆå†…ã®è¡¨ç¤ºã¯ã€ãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒauto/claude_cliã®ã¨ãã®ã¿
        # ï¼ˆgeminiã‚„openaiã‚’æ˜ç¤ºé¸æŠã—ã¦ã„ã‚‹å ´åˆã¯ãƒã‚¤ã‚ºã‚’å‡ºã•ãªã„ï¼‰
        if self.claude_code_available and not self.demo_mode:
            preferred = os.getenv("LLM_PROVIDER", "auto")
            if preferred in ("auto", "claude_cli"):
                print("ğŸ¤– Claude Codeå‘¼ã³å‡ºã—ãƒ¢ãƒ¼ãƒ‰: ç›´æ¥Claude Codeã‚’ä½¿ç”¨ã—ã¾ã™")
        elif not self.api_key or self.api_key in ["", "ere", "your-api-key-here"]:
            self.demo_mode = True
            print("ğŸ§ª GPT Demo Mode: ã‚¹ã‚¿ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã™")

    def _extract_json_object(self, text: str) -> Optional[str]:
        """Extract a JSON object substring from arbitrary text.
        Returns the JSON text between the first '{' and the last '}', if valid.
        """
        try:
            if not text:
                return None
            s = text.strip()
            # Strip common Markdown fences
            if s.startswith('```json') and s.endswith('```'):
                s = s[7:-3].strip()
            elif s.startswith('```') and s.endswith('```'):
                s = s[3:-3].strip()
            i, j = s.find('{'), s.rfind('}')
            if i >= 0 and j > i:
                return s[i:j+1]
            return None
        except Exception:
            return None
    
    def _check_claude_code(self) -> bool:
        """Claude CodeãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            # claude commandã®å­˜åœ¨ç¢ºèª
            result = subprocess.run(['which', 'claude'], capture_output=True, text=True, timeout=5)
            if result.returncode == 0:
                return True
            
            # ä¸€èˆ¬çš„ãªãƒ‘ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
            possible_paths = [
                '/usr/local/bin/claude',
                '/opt/homebrew/bin/claude',
                Path.home() / '.local' / 'bin' / 'claude'
            ]
            
            for path in possible_paths:
                if Path(path).exists():
                    return True
                
            return False
        except Exception:
            return False
    
    def _call_claude_code(self, prompt: str, system_prompt: str = "") -> Optional[str]:
        """Claude Codeã‚’ç›´æ¥å‘¼ã³å‡ºã—"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¿å­˜
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                if system_prompt:
                    f.write(f"System: {system_prompt}\n\nUser: {prompt}")
                else:
                    f.write(prompt)
                temp_file = f.name
            
            # Claude Codeã‚’å‘¼ã³å‡ºã—
            cmd = ['claude', '--file', temp_file, '--format', 'text']
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            os.unlink(temp_file)
            
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                print(f"Claude Code error: {result.stderr}")
                return None
                
        except Exception as e:
            print(f"Claude Codeå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _call_cli_json(self, prompt: str, system_prompt: str = "") -> Optional[str]:
        """Call external LLM CLI defined by LLM_CLI_CMD and return stdout text."""
        if not self.cli_cmd:
            return None
        try:
            # Write prompt to a temp file (include system prompt header if provided)
            with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
                if system_prompt:
                    f.write(f"System: {system_prompt}\n\nUser: {prompt}")
                else:
                    f.write(prompt)
                temp_file = f.name
            cmd = self.cli_cmd
            if '{file}' in cmd:
                cmd = cmd.replace('{file}', shlex.quote(temp_file))
            else:
                cmd = cmd + ' ' + shlex.quote(temp_file)
            # Execute via shell to allow pipelines/options
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=120)
            # Cleanup
            try:
                os.unlink(temp_file)
            except Exception:
                pass
            if result.returncode == 0:
                return result.stdout.strip()
            else:
                print(f"LLM_CLI_CMD failed: {result.stderr}")
                return None
        except Exception as e:
            print(f"CLI hook error: {e}")
            return None

    def _generate_with_gemini(self, prompt: str) -> Optional[str]:
        """Direct Gemini API call, returns response text if possible (JSON expected)."""
        try:
            import requests
            if not self.gemini_api_key:
                return None
            url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.gemini_model}:generateContent?key={self.gemini_api_key}"
            headers = {"Content-Type": "application/json"}
            data = {
                "contents": [{"parts": [{"text": prompt}]}],
                "generationConfig": {"temperature": 0.3}
            }
            r = requests.post(url, headers=headers, data=json.dumps(data), timeout=60)
            if r.status_code != 200:
                print(f"Gemini API error: {r.status_code} {r.text[:200]}")
                return None
            jr = r.json()
            # Extract text
            candidates = jr.get("candidates", [])
            if candidates and candidates[0].get("content", {}).get("parts"):
                parts = candidates[0]["content"]["parts"]
                text = ''.join(p.get("text", "") for p in parts)
                
                # Handle markdown-wrapped JSON (```json...```)
                text = text.strip()
                if text.startswith('```json') and text.endswith('```'):
                    # Remove markdown wrapper
                    text = text[7:-3].strip()
                elif text.startswith('```') and text.endswith('```'):
                    # Remove generic markdown wrapper
                    text = text[3:-3].strip()
                
                return text
            return None
        except Exception as e:
            print(f"Gemini request error: {e}")
            return None

    def split_text_for_subtitle(self, text: str, max_len: int = 26) -> Optional[List[str]]:
        """
        Use LLM to split Japanese text into natural subtitle lines.
        
        Args:
            text: Japanese text to split
            max_len: Maximum characters per line
            
        Returns:
            List of 1-2 strings, or None if API fails
        """
        if not self.api_key:
            return None
            
        prompt = f"""ã‚ãªãŸã¯æ—¥æœ¬èªå­—å¹•ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„å­—å¹•ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {text}

åˆ†å‰²è¦ä»¶:
â€¢ å„è¡Œ: æœ€å¤§{max_len}æ–‡å­—
â€¢ æœ€å¤§2è¡Œã¾ã§
â€¢ æ„å‘³ã®ã¾ã¨ã¾ã‚Šã‚’é‡è¦–
â€¢ ã€Œã€å†…ã‚„æ•°å€¤+å˜ä½ã¯åˆ†å‰²ç¦æ­¢
â€¢ è‡ªç„¶ãªèª­ã¿ãƒªã‚ºãƒ ã‚’ä¿æŒ
â€¢ æ–‡è„ˆã‚’å¤±ã‚ãªã„åˆ†å‰²

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{"lines": ["1è¡Œç›®", "2è¡Œç›®"]}}

å˜ä¸€è¡Œã®å ´åˆ:
{{"lines": ["å˜ä¸€è¡Œ"]}}

ä¾‹:
- å…¥åŠ›: "äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã«ã‚ˆã‚Šç§ãŸã¡ã®ç”Ÿæ´»ã¯å¤§ããå¤‰åŒ–ã—ã¦ã„ã¾ã™"
- å‡ºåŠ›: {{"lines": ["äººå·¥çŸ¥èƒ½ã®ç™ºå±•ã«ã‚ˆã‚Š", "ç§ãŸã¡ã®ç”Ÿæ´»ã¯å¤§ããå¤‰åŒ–ã—ã¦ã„ã¾ã™"]}}"""

        try:
            response = openai.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "æ—¥æœ¬èªå­—å¹•ã®å°‚é–€å®¶ã¨ã—ã¦ã€èª­ã¿ã‚„ã™ã•ã¨æ„å‘³ã®ä¿æŒã‚’ä¸¡ç«‹ã—ãŸè‡ªç„¶ãªåˆ†å‰²ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=300,
                temperature=0.2
            )
            
            content = response.choices[0].message.content.strip()
            result = json.loads(content)
            
            if "lines" in result and isinstance(result["lines"], list):
                # Validate line lengths
                lines = result["lines"]
                if all(len(line) <= max_len for line in lines) and len(lines) <= 2:
                    return lines
                    
        except json.JSONDecodeError as e:
            print(f"[WARN] LLM response JSON parsing failed: {e}")
            print(f"[DEBUG] Raw response: {content if 'content' in locals() else 'No response'}")
        except Exception as e:
            print(f"[WARN] LLM text splitting failed: {e}")
            
        return None

    def split_text_for_multiple_subtitles(self, text: str, max_len: int = 26) -> Optional[List[List[str]]]:
        """
        Use LLM to split Japanese text into multiple subtitle cards.
        
        Args:
            text: Japanese text to split
            max_len: Maximum characters per line
            
        Returns:
            List of subtitle cards (each card has 1-2 lines), or None if API fails
        """
        if not self.api_key:
            return None
            
        prompt = f"""æ—¥æœ¬èªå­—å¹•ã®å°‚é–€å®¶ã¨ã—ã¦ã€é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’è¤‡æ•°ã®å­—å¹•ã‚«ãƒ¼ãƒ‰ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {text}

åˆ†å‰²è¦ä»¶:
â€¢ 1ã‚«ãƒ¼ãƒ‰ = æœ€å¤§2è¡Œã€å„è¡Œ{max_len}æ–‡å­—ä»¥å†…
â€¢ æ„å‘³ã®å®Œçµæ€§ã‚’é‡è¦–ï¼ˆæ–‡è„ˆã‚’ä¿æŒï¼‰
â€¢ æƒ…å ±ã®çœç•¥ãƒ»è¦ç´„ç¦æ­¢ï¼ˆå…¨å†…å®¹ã‚’ä¿æŒï¼‰
â€¢ ã€Œã€å†…ã€æ•°å€¤+å˜ä½ã€å›ºæœ‰åè©ã¯åˆ†å‰²ç¦æ­¢
â€¢ è‡ªç„¶ãªèª­ã¿ãƒªã‚ºãƒ ã¨ç†è§£ã—ã‚„ã™ã•

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{"cards": [
  ["ã‚«ãƒ¼ãƒ‰1è¡Œ1", "ã‚«ãƒ¼ãƒ‰1è¡Œ2"],
  ["ã‚«ãƒ¼ãƒ‰2è¡Œ1"],
  ["ã‚«ãƒ¼ãƒ‰3è¡Œ1", "ã‚«ãƒ¼ãƒ‰3è¡Œ2"]
]}}

ä¾‹:
- çŸ­æ–‡: {{"cards": [["çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆ"]]}}
- é•·æ–‡: {{"cards": [["å‰åŠã®æ„å‘³", "ã¾ã¨ã¾ã‚Š"], ["å¾ŒåŠã®æ„å‘³", "ã¾ã¨ã¾ã‚Š"]]}}

é‡è¦: å…¨ã¦ã®æƒ…å ±ã‚’ä¿æŒã—ã€èª­ã¿ã‚„ã™ãåˆ†å‰²ã—ã¦ãã ã•ã„ã€‚"""

        try:
            response = openai.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "æ—¥æœ¬èªå­—å¹•ã®å°‚é–€å®¶ã¨ã—ã¦ã€å…¨ã¦ã®æƒ…å ±ã‚’ä¿æŒã—ãªãŒã‚‰è‡ªç„¶ã§ç†è§£ã—ã‚„ã™ã„åˆ†å‰²ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚çœç•¥ã¯ä¸€åˆ‡ã—ãªã„ã§ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=800,
                temperature=0.2
            )
            
            content = response.choices[0].message.content.strip()
            result = json.loads(content)
            
            if "cards" in result and isinstance(result["cards"], list):
                cards = result["cards"]
                # Validate each card
                valid_cards = []
                for card in cards:
                    if isinstance(card, list) and len(card) <= 2:
                        if all(isinstance(line, str) and len(line) <= max_len for line in card):
                            valid_cards.append(card)
                
                if valid_cards:
                    return valid_cards
                    
        except json.JSONDecodeError as e:
            print(f"[WARN] LLM multiple subtitle response JSON parsing failed: {e}")
            print(f"[DEBUG] Raw response: {content if 'content' in locals() else 'No response'}")
        except Exception as e:
            print(f"[WARN] LLM multiple subtitle splitting failed: {e}")
            
        return None

    def split_text_to_cards(self, text: str, max_len: int = 26, max_lines_per_card: int = 2) -> Optional[List[List[str]]]:
        """
        Provider-agnostic subtitle card splitter with cost tracking.
        Tries CLI -> Claude -> Gemini -> OpenAI -> None.
        Returns list of cards (each 1-2 lines) or None.
        """
        # Build prompt shared across providers
        prompt = f"""ã‚ãªãŸã¯æ—¥æœ¬èªå­—å¹•ã®ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ„å‘³ã‚’å¤±ã‚ãšã«è‡ªç„¶ãªå­—å¹•ã‚«ãƒ¼ãƒ‰ã«åˆ†å‰²ã—ã¦ãã ã•ã„ã€‚

ãƒ†ã‚­ã‚¹ãƒˆ: {text}

è¦ä»¶:
â€¢ 1ã‚«ãƒ¼ãƒ‰ = æœ€å¤§{max_lines_per_card}è¡Œã€å„è¡Œ{max_len}æ–‡å­—ä»¥å†…
â€¢ çœç•¥ã‚„è¦ç´„ã¯ç¦æ­¢ã€‚å…¨æƒ…å ±ã‚’ä¿æŒ
â€¢ è‡ªç„¶ãªèª­ç‚¹/èªå°¾ã§æŠ˜ã‚Šè¿”ã—ã€èª­ã¿ã‚„ã™ã•é‡è¦–
â€¢ JSONã®ã¿ã‚’è¿”ã™

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{"cards": [["è¡Œ1", "è¡Œ2"], ["è¡Œ1"], ["è¡Œ1", "è¡Œ2"]]}}
"""

        # Preferred path: respect provider setting
        print(f"[LLM] split_text_to_cards start provider={self.preferred_provider} len={len(text)} max_len={max_len}")
        _t0 = None
        try:
            import time as _time
            _t0 = _time.time()
        except Exception:
            pass
        if self.preferred_provider == 'cli' and self.cli_cmd:
            out = self._call_cli_json(prompt)
            if out:
                try:
                    data = json.loads(out)
                    cards = data.get("cards")
                    if isinstance(cards, list):
                        try:
                            rec = self.llm_cost.estimate("cli", "unknown", len(prompt), len(out))
                            self.last_cost = rec
                        except Exception:
                            pass
                        return cards
                except Exception as e:
                    print(f"CLI cards JSON parse error: {e}")

        if self.preferred_provider == 'claude_cli' and self.claude_code_available:
            out = self._call_claude_code(prompt)
            if out:
                try:
                    # Extract JSON substring
                    i, j = out.find('{'), out.rfind('}')
                    if i >= 0 and j > i:
                        json_text = out[i:j+1]
                        data = json.loads(json_text)
                        cards = data.get("cards")
                        if isinstance(cards, list):
                            try:
                                rec = self.llm_cost.estimate("claude", "unknown", len(prompt), len(json_text))
                                self.last_cost = rec
                            except Exception:
                                pass
                            return cards
                except Exception as e:
                    print(f"Claude cards JSON parse error: {e}")

        if self.preferred_provider == 'gemini' and self.gemini_api_key:
            out = self._generate_with_gemini(prompt)
            if out:
                try:
                    json_text = self._extract_json_object(out) or out
                    data = json.loads(json_text)
                    cards = data.get("cards")
                    if isinstance(cards, list):
                        try:
                            rec = self.llm_cost.estimate("gemini", self.gemini_model, len(prompt), len(json_text))
                            self.last_cost = rec
                        except Exception:
                            pass
                        return cards
                except Exception as e:
                    print(f"Gemini cards JSON parse error: {e}")
                    # Dump raw for debugging
                    try:
                        log_dir = Path("output/logs"); log_dir.mkdir(parents=True, exist_ok=True)
                        (log_dir / "llm_gemini_cards_error.json").write_text(out, encoding="utf-8")
                    except Exception:
                        pass

        if self.preferred_provider in ('openai','auto') and self.api_key and openai is not None:
            try:
                response = openai.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "æ—¥æœ¬èªå­—å¹•ã®å°‚é–€å®¶ã¨ã—ã¦ã€è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„å­—å¹•ã‚«ãƒ¼ãƒ‰åˆ†å‰²ã®ã¿JSONã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=800,
                    temperature=0.2
                )
                content = response.choices[0].message.content.strip()
                data = json.loads(content)
                cards = data.get("cards")
                if isinstance(cards, list):
                    try:
                        rec = self.llm_cost.estimate("openai", self.model, len(prompt), len(content))
                        self.last_cost = rec
                    except Exception:
                        pass
                    return cards
            except Exception as e:
                print(f"OpenAI cards error: {e}")

        # Auto mode fallbacks if specific provider paths failed
        if self.preferred_provider == 'auto':
            # Try CLI
            if self.cli_cmd:
                out = self._call_cli_json(prompt)
                if out:
                    try:
                        data = json.loads(out)
                        cards = data.get("cards")
                        if isinstance(cards, list):
                            try:
                                rec = self.llm_cost.estimate("cli", "unknown", len(prompt), len(out))
                                self.last_cost = rec
                            except Exception:
                                pass
                            return cards
                    except Exception as e:
                        print(f"CLI cards JSON parse error: {e}")
            # Try Claude
            if self.claude_code_available:
                out = self._call_claude_code(prompt)
                if out:
                    try:
                        i, j = out.find('{'), out.rfind('}')
                        if i >= 0 and j > i:
                            json_text = out[i:j+1]
                            data = json.loads(json_text)
                            cards = data.get("cards")
                            if isinstance(cards, list):
                                try:
                                    rec = self.llm_cost.estimate("claude", "unknown", len(prompt), len(json_text))
                                    self.last_cost = rec
                                except Exception:
                                    pass
                                return cards
                    except Exception as e:
                        print(f"Claude cards JSON parse error: {e}")
            # Try Gemini
            if self.gemini_api_key:
                out = self._generate_with_gemini(prompt)
                if out:
                    try:
                        json_text = self._extract_json_object(out) or out
                        data = json.loads(json_text)
                        cards = data.get("cards")
                        if isinstance(cards, list):
                            try:
                                rec = self.llm_cost.estimate("gemini", self.gemini_model, len(prompt), len(json_text))
                                self.last_cost = rec
                            except Exception:
                                pass
                            return cards
                    except Exception as e:
                        print(f"Gemini cards JSON parse error: {e}")
                        try:
                            log_dir = Path("output/logs"); log_dir.mkdir(parents=True, exist_ok=True)
                            (log_dir / "llm_gemini_cards_error_auto.json").write_text(out, encoding="utf-8")
                        except Exception:
                            pass
        # Timing end
        try:
            if _t0 is not None:
                import time as _time
                print(f"[LLM] split_text_to_cards end elapsed={_time.time()-_t0:.2f}s -> None")
        except Exception:
            pass
        return None

    # === ElevenLabs v3 Audio-tag Enhancement ===
    def enhance_tts_with_eleven_v3(self, text: str, role: str = "NA", speaker: Optional[str] = None) -> Optional[str]:
        """
        Generate audio-tagged (ElevenLabs v3) performance text from a script line.

        Args:
            text: Original script line
            role: 'NA' (narration) or 'DL' (dialogue) or 'QT'
            speaker: Optional speaker name to prefix for dialogue

        Returns:
            Tag-enhanced text (plain string with tags) or None on failure
        """
        role = (role or "NA").upper()
        speaker_hint = (speaker or ("Narrator" if role == "NA" else "Speaker")).strip()
        # System + user prompt crafted for ElevenLabs v3 audio tags
        sys_prompt = (
            "ã‚ãªãŸã¯éŸ³å£°æ¼”å‡ºå®¶ã§ã™ã€‚æ—¥æœ¬èªå°æœ¬ã«å¯¾ã—ã€ElevenLabs v3ã®éŸ³å£°ã‚¿ã‚°ã‚’é©åˆ‡ã«æŒ¿å…¥ã—ã¦ã€"
            "æ„Ÿæƒ…ãƒ»é–“ãƒ»éè¨€èªè¡¨ç¾ï¼ˆæ¯é£ã„ãƒ»ç¬‘ã„ç­‰ï¼‰ã‚’æ¼”å‡ºã—ã¾ã™ã€‚å‡ºåŠ›ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã€‚èª¬æ˜ã‚„JSONã¯ä¸è¦ã€‚"
        )
        user_prompt = f"""
ä»¥ä¸‹ã®å°æœ¬è¡Œã«ã€ElevenLabs v3ã§ä½¿ç”¨ã§ãã‚‹éŸ³å£°ã‚¿ã‚°ã‚’ä»˜åŠ ã—ã¦ãã ã•ã„ã€‚

ãƒ«ãƒ¼ãƒ«:
- <laugh>, <sigh>, <shout>, <whisper> ãªã©ã®éè¨€èªã‚¿ã‚°ã‚’å¿…è¦ã«å¿œã˜ã¦æŒ¿å…¥
- æ„Ÿæƒ…ãƒ»ãƒˆãƒ¼ãƒ³ã‚’ <emotional mood="...">ã€œ</emotional> ã§åŒ…ã‚€ï¼ˆä¾‹: calm, excited, serious ãªã©ï¼‰
- ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯ <narration>ã€œ</narration> ã¨ã—ã€è½ã¡ç€ã„ãŸãƒˆãƒ¼ãƒ³ï¼ˆcalmï¼‰ã‚’åŸºæœ¬ã«
- ã‚»ãƒªãƒ•ï¼ˆå¯¾è©±ï¼‰ã¯è¡Œé ­ã«è©±è€…åã‚’ä»˜ä¸ï¼ˆä¾‹: {speaker_hint}: ...ï¼‰
- åŸæ–‡ã®æƒ…å ±ã¯çœç•¥ã›ãšä¿æŒã€‚éå‰°ãªè„šè‰²ã¯é¿ã‘ã‚‹
- å‡ºåŠ›ã¯éŸ³å£°ã‚¿ã‚°ä»˜ãã®ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ï¼ˆMarkdownã‚„èª¬æ˜ã¯ä¸è¦ï¼‰

å½¹å‰²: {('ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³' if role=='NA' else 'ã‚»ãƒªãƒ•')}
è©±è€…å: {speaker_hint}
ãƒ†ã‚­ã‚¹ãƒˆ: {text}
"""

        # Provider preference: try CLI, then Claude Code, then Gemini, then OpenAI
        # CLI
        if self.preferred_provider == 'cli' and self.cli_cmd:
            out = self._call_cli_json(user_prompt, system_prompt=sys_prompt)
            if out:
                return out.strip()

        # Claude Code (text output)
        if self.preferred_provider == 'claude_cli' and self.claude_code_available:
            out = self._call_claude_code(user_prompt, system_prompt=sys_prompt)
            if out:
                # Basic sanitization: strip code fences
                s = out.strip()
                if s.startswith('```') and s.endswith('```'):
                    s = s.split('\n', 1)[1].rsplit('\n', 1)[0]
                try:
                    rec = self.llm_cost.estimate("claude", "unknown", len(user_prompt), len(s))
                    self.last_cost = rec
                except Exception:
                    pass
                return s

        # Gemini
        if self.preferred_provider == 'gemini' and self.gemini_api_key:
            composed = f"System: {sys_prompt}\n\nUser: {user_prompt}"
            out = self._generate_with_gemini(composed)
            if out:
                s = (out or '').strip()
                # Remove Markdown fences if present
                if s.startswith('```') and s.endswith('```'):
                    s = s.split('\n', 1)[1].rsplit('\n', 1)[0]
                try:
                    rec = self.llm_cost.estimate("gemini", self.gemini_model, len(composed), len(s))
                    self.last_cost = rec
                except Exception:
                    pass
                return s

        # OpenAI fallback
        if (self.preferred_provider in ('openai', 'auto')) and (self.api_key and openai is not None):
            try:
                response = openai.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": sys_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    max_tokens=800,
                    temperature=0.3
                )
                s = response.choices[0].message.content.strip()
                # Strip fences if any
                if s.startswith('```') and s.endswith('```'):
                    s = s.split('\n', 1)[1].rsplit('\n', 1)[0]
                try:
                    rec = self.llm_cost.estimate("openai", self.model, len(user_prompt), len(s))
                    self.last_cost = rec
                except Exception:
                    pass
                return s
            except Exception as e:
                print(f"[WARN] OpenAI enhance_tts failed: {e}")

        return None

    def generate_storyboard(self, script_text: str, parsed_segments: List = None) -> Optional[dict]:
        """
        Generate storyboard with scene breakdown, shot descriptions, and stock keywords.
        
        Args:
            script_text: Japanese script text to analyze
            parsed_segments: Pre-analyzed script segments (optional)
            
        Returns:
            Dictionary with storyboard structure, or None if API fails
        """
        # Demoç”Ÿæˆã¯ç„¡åŠ¹åŒ–ï¼ˆLLMä»¥å¤–ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å»ƒæ­¢ï¼‰

        prompt = f"""ã‚ãªãŸã¯æ˜ åƒåˆ¶ä½œã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å°æœ¬ã‹ã‚‰æ–‡å­—ã‚³ãƒ³ãƒ†ï¼ˆã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å°æœ¬:
{script_text}

è¦æ±‚:
1. å°æœ¬ã‚’æ„å‘³çš„ãªã‚·ãƒ¼ãƒ³ã«åˆ†å‰²
2. å„ã‚·ãƒ¼ãƒ³ã®æ¦‚è¦ã‚’1æ–‡ã§è¦ç´„
3. é©åˆ‡ãªã‚·ãƒ§ãƒƒãƒˆï¼ˆã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ï¼‰ã‚’ææ¡ˆ
4. ç´ ææ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‹±èªãƒ»æ—¥æœ¬èªã§ç”Ÿæˆ
5. AIç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ

ã‚·ãƒ§ãƒƒãƒˆè¨˜å·:
- WS (Wide Shot): å…¨æ™¯ãƒ»é¢¨æ™¯
- MS (Medium Shot): ä¸­æ™¯ãƒ»äººç‰©è…°ä¸Š
- CU (Close Up): ã‚¯ãƒ­ãƒ¼ã‚ºã‚¢ãƒƒãƒ—ãƒ»è¡¨æƒ…
- POV (Point of View): ä¸»è¦³ã‚·ãƒ§ãƒƒãƒˆ
- INSERT: æŒ¿å…¥ã‚«ãƒƒãƒˆãƒ»è³‡æ–™ãªã©

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{
  "storyboard": [
    {{
      "scene_id": "SC-001",
      "outline": "ã‚·ãƒ¼ãƒ³ã®1æ–‡è¦ç´„",
      "shotlist": [
        {{"shot": "WS", "desc": "å…·ä½“çš„ãªã‚·ãƒ§ãƒƒãƒˆèª¬æ˜"}}
      ],
      "stock_keywords": ["english keyword", "japanese keyword", "concept"],
      "gen_prompts": ["AIç”»åƒç”Ÿæˆç”¨ã®è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
    }}
  ]
}}

æ³¨æ„:
- å„ã‚·ãƒ¼ãƒ³ã¯å°æœ¬ã®è‡ªç„¶ãªæ„å‘³çš„åŒºåˆ‡ã‚Šã«å¾“ã†
- ã‚·ãƒ§ãƒƒãƒˆèª¬æ˜ã¯å…·ä½“çš„ã§æ˜ åƒçš„ã«
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ç´ æã‚µã‚¤ãƒˆæ¤œç´¢ã«æœ€é©åŒ–
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯é«˜å“è³ªãªç”»åƒç”Ÿæˆã‚’æƒ³å®š"""

        # Provider preference
        if self.preferred_provider == 'cli' and self.cli_cmd:
            response = self._call_cli_json(prompt)
            if response:
                try:
                    result = json.loads(response)
                    if "storyboard" in result and isinstance(result["storyboard"], list):
                        # Cost estimate (provider unknown CLI)
                        try:
                            rec = self.llm_cost.estimate("cli", "unknown", len(prompt), len(response))
                            self.last_cost = rec
                            print(f"[COST] LLM est (CLI): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                        except Exception:
                            pass
                        return result
                except Exception as e:
                    print(f"CLI JSON parse error: {e}")
        
        if self.preferred_provider == 'claude_cli' and self.claude_code_available:
            result = self._generate_storyboard_with_claude(script_text, parsed_segments)
            if result:
                # Rough cost estimate for Claude path (no usage data available)
                try:
                    prompt_chars = len(prompt)
                    completion_chars = len(json.dumps(result, ensure_ascii=False))
                    rec = self.llm_cost.estimate("claude", "unknown", prompt_chars, completion_chars)
                    self.last_cost = rec
                    print(f"[COST] LLM est (Claude): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                except Exception:
                    pass
                return result
            print("[WARN] Claude storyboard generation failed")
            return None

        if self.preferred_provider == 'gemini' and self.gemini_api_key:
            print("[LLM] generate_storyboard via Gemini")
            response = self._generate_with_gemini(prompt)
            if response:
                try:
                    json_text = self._extract_json_object(response) or response
                    result = json.loads(json_text)
                    if "storyboard" in result and isinstance(result["storyboard"], list):
                        try:
                            rec = self.llm_cost.estimate("gemini", self.gemini_model, len(prompt), len(json_text))
                            self.last_cost = rec
                            print(f"[COST] LLM est (Gemini): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                        except Exception:
                            pass
                        return result
                except Exception as e:
                    print(f"Gemini JSON parse error: {e}")
                    try:
                        log_dir = Path("output/logs"); log_dir.mkdir(parents=True, exist_ok=True)
                        (log_dir / "llm_gemini_storyboard_error.json").write_text(response, encoding="utf-8")
                    except Exception:
                        pass

        if self.preferred_provider == 'openai' and (not self.api_key or openai is None):
            return None
            
        try:
            # 'auto' mode will try OpenAI as a last resort
            if self.preferred_provider == 'openai' or (self.preferred_provider == 'auto' and self.api_key and openai is not None):
                response = openai.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "æ˜ åƒåˆ¶ä½œã¨ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ä½œæˆã®å°‚é–€å®¶ã¨ã—ã¦ã€å®Ÿç”¨çš„ã§è©³ç´°ãªæ–‡å­—ã‚³ãƒ³ãƒ†ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=2000,
                    temperature=0.3
                )
                content = response.choices[0].message.content.strip()
                result = json.loads(content)
                
                if "storyboard" in result and isinstance(result["storyboard"], list):
                    # Cost estimate for OpenAI path
                    try:
                        rec = self.llm_cost.estimate("openai", self.model, len(prompt), len(content))
                        self.last_cost = rec
                        print(f"[COST] LLM est (OpenAI): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                    except Exception:
                        pass
                    return result
                print("[WARN] Storyboard response invalid")
                return None
            
            # If we are in 'auto' mode and got here, it means all preferred providers failed.
            # The logic above already tried OpenAI if it was available.
            if self.preferred_provider == 'auto':
                print("[WARN] All LLM providers failed for storyboard generation.")
                return None

        except json.JSONDecodeError as e:
            print(f"[WARN] Storyboard response JSON parsing failed: {e}")
            print(f"[DEBUG] Raw response: {content if 'content' in locals() else 'No response'}")
            return None
        except Exception as e:
            print(f"[WARN] Storyboard generation failed: {e}")
            return None
        
        return None

    def generate_music_prompts(self, script_text: str, parsed_segments: List = None) -> Optional[dict]:
        """
        Generate BGM atmosphere descriptions and music prompts.
        
        Args:
            script_text: Japanese script text to analyze
            
        Returns:
            Dictionary with music prompts structure, or None if API fails
        """
        # Demoç”Ÿæˆã¯ç„¡åŠ¹åŒ–

        prompt = f"""ã‚ãªãŸã¯éŸ³æ¥½åˆ¶ä½œã¨æ˜ åƒéŸ³éŸ¿ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®å°æœ¬ã‹ã‚‰BGMãƒ»éŸ³æ¥½ã®é›°å›²æ°—è¨˜è¿°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å°æœ¬:
{script_text}

è¦æ±‚:
1. å°æœ¬ã®å†…å®¹ãƒ»é›°å›²æ°—ã«åˆã£ãŸBGMã‚­ãƒ¥ãƒ¼ã‚’ææ¡ˆ
2. å„ã‚­ãƒ¥ãƒ¼ã®æ„Ÿæƒ…ãƒ»ãƒ ãƒ¼ãƒ‰ãƒ»BPMãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æŒ‡å®š
3. Suno AIç­‰ã®éŸ³æ¥½ç”ŸæˆAIã«é©ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
4. ç´ æã‚µã‚¤ãƒˆæ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ

éŸ³æ¥½ã‚¹ã‚¿ã‚¤ãƒ«ä¾‹:
- ambient, cinematic, piano, strings, electronic
- uplifting, contemplative, mysterious, warm, energetic
- documentary style, meditation music, corporate

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{
  "music_prompts": [
    {{
      "cue_id": "M-001",
      "mood": "warm introspective",
      "bpm": 78,
      "style": "ambient piano with subtle strings",
      "suno_prompt": "Gentle contemplative piano melody with warm ambient textures, soft string pads, 78 BPM, documentary style, meditation mood",
      "stock_keywords": ["contemplative piano", "ambient documentary", "meditation background"]
    }}
  ]
}}

æ³¨æ„:
- å°æœ¬ã®æ„Ÿæƒ…çš„ãªæµã‚Œã‚’éŸ³æ¥½ã§è¡¨ç¾
- å„ã‚­ãƒ¥ãƒ¼ã¯3-5åˆ†ç¨‹åº¦ã®æ¥½æ›²ã‚’æƒ³å®š
- Suno AIãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å…·ä½“çš„ã§è©³ç´°ã«
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯éŸ³æ¥½ç´ æã‚µã‚¤ãƒˆæ¤œç´¢ã«æœ€é©åŒ–"""

        if self.preferred_provider == 'cli' and self.cli_cmd:
            response = self._call_cli_json(prompt)
            if response:
                try:
                    result = json.loads(response)
                    if "music_prompts" in result and isinstance(result["music_prompts"], list):
                        try:
                            rec = self.llm_cost.estimate("cli", "unknown", len(prompt), len(response))
                            self.last_cost = rec
                            print(f"[COST] LLM est (CLI): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                        except Exception:
                            pass
                        return result
                except Exception as e:
                    print(f"CLI JSON parse error: {e}")
        
        if self.preferred_provider == 'claude_cli' and self.claude_code_available:
            result = self._generate_music_prompts_with_claude(script_text, parsed_segments)
            if result:
                try:
                    prompt_chars = len(prompt)
                    completion_chars = len(json.dumps(result, ensure_ascii=False))
                    rec = self.llm_cost.estimate("claude", "unknown", prompt_chars, completion_chars)
                    self.last_cost = rec
                    print(f"[COST] LLM est (Claude): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                except Exception:
                    pass
                return result
            print("[WARN] Claude music prompts failed")
            return None

        if self.preferred_provider == 'gemini' and self.gemini_api_key:
            print("[LLM] generate_music_prompts via Gemini")
            response = self._generate_with_gemini(prompt)
            if response:
                try:
                    json_text = self._extract_json_object(response) or response
                    result = json.loads(json_text)
                    if "music_prompts" in result and isinstance(result["music_prompts"], list):
                        try:
                            rec = self.llm_cost.estimate("gemini", self.gemini_model, len(prompt), len(json_text))
                            self.last_cost = rec
                            print(f"[COST] LLM est (Gemini): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                        except Exception:
                            pass
                        return result
                except Exception as e:
                    print(f"Gemini JSON parse error: {e}")
                    try:
                        log_dir = Path("output/logs"); log_dir.mkdir(parents=True, exist_ok=True)
                        (log_dir / "llm_gemini_music_error.json").write_text(response, encoding="utf-8")
                    except Exception:
                        pass

        if self.preferred_provider == 'openai' and (not self.api_key or openai is None):
            return None
            
        try:
            if self.preferred_provider == 'openai' or (self.preferred_provider == 'auto' and self.api_key and openai is not None):
                response = openai.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "éŸ³æ¥½åˆ¶ä½œã¨æ˜ åƒéŸ³éŸ¿ã®å°‚é–€å®¶ã¨ã—ã¦ã€å°æœ¬ã®æ„Ÿæƒ…çš„ãªæµã‚Œã«åˆã£ãŸé©åˆ‡ãªéŸ³æ¥½ææ¡ˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=1500,
                    temperature=0.4
                )
                content = response.choices[0].message.content.strip()
                result = json.loads(content)
                
                if "music_prompts" in result and isinstance(result["music_prompts"], list):
                    try:
                        rec = self.llm_cost.estimate("openai", self.model, len(prompt), len(content))
                        self.last_cost = rec
                        print(f"[COST] LLM est (OpenAI): Â¥{rec['cost_jpy']} (${rec['cost_usd']})")
                    except Exception:
                        pass
                    return result
                print("[WARN] Music prompts response invalid")
                return None
            
            if self.preferred_provider == 'auto':
                print("[WARN] All LLM providers failed for music prompt generation.")
                return None

        except json.JSONDecodeError as e:
            print(f"[WARN] Music prompts response JSON parsing failed: {e}")
            print(f"[DEBUG] Raw response: {content if 'content' in locals() else 'No response'}")
            return None
        except Exception as e:
            print(f"[WARN] Music prompts generation failed: {e}")
            return None
        
        return None

    def _generate_demo_storyboard(self, script_text: str) -> dict:
        """ãƒ‡ãƒ¢ç”¨æ–‡å­—ã‚³ãƒ³ãƒ†ç”Ÿæˆ"""
        # å°æœ¬ã®é•·ã•ã«å¿œã˜ã¦ã‚·ãƒ¼ãƒ³æ•°ã‚’æ±ºå®š
        lines = script_text.split('\n')
        scene_count = max(3, min(8, len(lines) // 3))
        
        scenes = []
        for i in range(scene_count):
            scene_id = f"SC-{i+1:03d}"
            
            if i == 0:
                outline = "ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ãƒ»å°å…¥ã‚·ãƒ¼ãƒ³"
                shot_desc = "æ›¸æ–ã®å…¨æ™¯ã€æœ¬æ£šã¨æœº"
                keywords = ["study room", "books library", "academic workspace", "æ›¸æ–", "æœ¬æ£š", "å­¦ç¿’ç’°å¢ƒ"]
            elif i == scene_count - 1:
                outline = "ã¾ã¨ã‚ãƒ»ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ³ã‚°"
                shot_desc = "å¤•æ—¥ã®ä¸­ã®é¢¨æ™¯ã€å¸Œæœ›çš„ãªé›°å›²æ°—"
                keywords = ["sunset horizon", "hopeful future", "peaceful landscape", "å¤•æ—¥", "å¸Œæœ›", "æœªæ¥"]
            else:
                outline = f"ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ - ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {i}"
                shot_desc = f"å°‚é–€å®¶ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã€è³‡æ–™æ˜ åƒ"
                keywords = ["expert interview", "documentary style", "professional portrait", "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "å°‚é–€å®¶", "è§£èª¬"]
            
            scenes.append({
                "scene_id": scene_id,
                "outline": outline,
                "shotlist": [{"shot": "WS" if i % 3 == 0 else "MS" if i % 3 == 1 else "CU", "desc": shot_desc}],
                "stock_keywords": keywords,
                "gen_prompts": [f"Professional {shot_desc.lower()}, documentary style, high quality, cinematic lighting"]
            })
        
        return {"storyboard": scenes}
    
    def _generate_demo_music_prompts(self, script_text: str) -> dict:
        """ãƒ‡ãƒ¢ç”¨BGMé›°å›²æ°—ç”Ÿæˆ"""
        # å°æœ¬ã®é›°å›²æ°—ã‚’ç°¡æ˜“åˆ†æ
        music_cues = []
        
        # åŸºæœ¬çš„ãªBGMã‚­ãƒ¥ãƒ¼ã‚’ç”Ÿæˆ
        cues_data = [
            {
                "cue_id": "M-001",
                "mood": "contemplative and warm",
                "bpm": 72,
                "style": "ambient piano with subtle strings",
                "suno_prompt": "Gentle contemplative piano melody with warm ambient textures, soft string pads, 72 BPM, documentary style, introspective mood",
                "stock_keywords": ["contemplative piano", "ambient documentary", "warm introspective"]
            },
            {
                "cue_id": "M-002", 
                "mood": "inspiring and uplifting",
                "bpm": 85,
                "style": "orchestral with piano lead",
                "suno_prompt": "Inspiring orchestral arrangement with piano melody, building emotional crescendo, 85 BPM, hopeful and uplifting",
                "stock_keywords": ["inspiring orchestral", "uplifting piano", "emotional crescendo"]
            },
            {
                "cue_id": "M-003",
                "mood": "peaceful resolution",
                "bpm": 65,
                "style": "solo piano with ambient pad",
                "suno_prompt": "Peaceful solo piano with gentle ambient background, resolution and closure feeling, 65 BPM, calming finish",
                "stock_keywords": ["peaceful piano", "ambient resolution", "calming outro"]
            }
        ]
        
        return {"music_prompts": cues_data}

    def _generate_storyboard_with_claude(self, script_text: str, parsed_segments: List = None) -> Optional[dict]:
        """Claude Codeã‚’ä½¿ã£ã¦æ–‡å­—ã‚³ãƒ³ãƒ†ç”Ÿæˆï¼ˆè§£æçµæœé€£æºç‰ˆï¼‰"""
        system_prompt = """ã‚ãªãŸã¯æ˜ åƒåˆ¶ä½œã®å°‚é–€å®¶ã§ã™ã€‚å°æœ¬è§£æçµæœã‚’æ´»ç”¨ã—ã¦é«˜å“è³ªãªæ–‡å­—ã‚³ãƒ³ãƒ†ï¼ˆã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ï¼‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã‚·ãƒ§ãƒƒãƒˆè¨˜å·:
- WS (Wide Shot): å…¨æ™¯ãƒ»é¢¨æ™¯ãƒ»ç’°å¢ƒã‚·ãƒ§ãƒƒãƒˆ
- MS (Medium Shot): ä¸­æ™¯ãƒ»äººç‰©è…°ä¸Šãƒ»ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼
- CU (Close Up): ã‚¯ãƒ­ãƒ¼ã‚ºã‚¢ãƒƒãƒ—ãƒ»è¡¨æƒ…ãƒ»é‡è¦ãªè©³ç´°
- POV (Point of View): ä¸»è¦³ã‚·ãƒ§ãƒƒãƒˆãƒ»è¦–ç‚¹åˆ‡ã‚Šæ›¿ãˆ
- INSERT: æŒ¿å…¥ã‚«ãƒƒãƒˆãƒ»è³‡æ–™ãƒ»ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯

è©±è€…ã‚¿ã‚¤ãƒ—åˆ¥ã‚·ãƒ§ãƒƒãƒˆææ¡ˆ:
- ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³(NA): WSç’°å¢ƒã‚·ãƒ§ãƒƒãƒˆã€INSERTè³‡æ–™æ˜ åƒãŒåŠ¹æœçš„
- å¯¾è©±ãƒ»ã‚»ãƒªãƒ•(DL): MSäººç‰©ã‚·ãƒ§ãƒƒãƒˆã€CUè¡¨æƒ…ã‚·ãƒ§ãƒƒãƒˆãŒåŠ¹æœçš„
- CPSé«˜é€Ÿ(>10): çŸ­ã„ã‚«ãƒƒãƒˆã€ã‚·ãƒ³ãƒ—ãƒ«ãªæ˜ åƒæ§‹æˆãŒæ¨å¥¨

å¿…ãšJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè§£æçµæœã‚’å«ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        analysis_info = ""
        if parsed_segments:
            analysis_info = "\n\n## å°æœ¬è§£æçµæœï¼ˆå‚è€ƒæƒ…å ±ï¼‰:\n"
            for i, segment in enumerate(parsed_segments[:10], 1):  # æœ€åˆã®10ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ã¿
                speaker = getattr(segment, 'speaker_type', 'UNKNOWN')
                text = getattr(segment, 'text', '')[:100]  # æœ€åˆã®100æ–‡å­—
                analysis_info += f"ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ{i}: [{speaker.value if hasattr(speaker, 'value') else speaker}] {text}...\n"
            analysis_info += "\nâ€» ã“ã®è§£æçµæœã‚’å‚è€ƒã«ã€è©±è€…ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé©åˆ‡ãªã‚·ãƒ§ãƒƒãƒˆæ§‹æˆã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"

        prompt = f"""ä»¥ä¸‹ã®å°æœ¬ã‹ã‚‰æ–‡å­—ã‚³ãƒ³ãƒ†ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

å°æœ¬:
{script_text}{analysis_info}

è¦æ±‚:
1. å°æœ¬ã‚’æ„å‘³çš„ãªã‚·ãƒ¼ãƒ³ã«åˆ†å‰²
2. å„ã‚·ãƒ¼ãƒ³ã®æ¦‚è¦ã‚’1æ–‡ã§è¦ç´„
3. é©åˆ‡ãªã‚·ãƒ§ãƒƒãƒˆï¼ˆã‚«ãƒ¡ãƒ©ã‚¢ãƒ³ã‚°ãƒ«ï¼‰ã‚’ææ¡ˆ
4. ç´ ææ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è‹±èªãƒ»æ—¥æœ¬èªã§ç”Ÿæˆ
5. AIç”»åƒç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{
  "storyboard": [
    {{
      "scene_id": "SC-001",
      "outline": "ã‚·ãƒ¼ãƒ³ã®1æ–‡è¦ç´„",
      "shotlist": [
        {{"shot": "WS", "desc": "å…·ä½“çš„ãªã‚·ãƒ§ãƒƒãƒˆèª¬æ˜"}}
      ],
      "stock_keywords": ["english keyword", "japanese keyword", "concept"],
      "gen_prompts": ["AIç”»åƒç”Ÿæˆç”¨ã®è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
    }}
  ]
}}"""

        response = self._call_claude_code(prompt, system_prompt)
        if response:
            try:
                # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_text = response[json_start:json_end]
                    return json.loads(json_text)
            except json.JSONDecodeError as e:
                print(f"Claude Code JSON parse error: {e}")
                return None
        
        return None

    def _generate_music_prompts_with_claude(self, script_text: str, parsed_segments: List = None) -> Optional[dict]:
        """Claude Codeã‚’ä½¿ã£ã¦BGMé›°å›²æ°—ç”Ÿæˆï¼ˆè§£æçµæœé€£æºç‰ˆï¼‰"""
        system_prompt = """ã‚ãªãŸã¯éŸ³æ¥½åˆ¶ä½œã¨æ˜ åƒéŸ³éŸ¿ã®å°‚é–€å®¶ã§ã™ã€‚å°æœ¬è§£æçµæœã‚’æ´»ç”¨ã—ã¦BGMãƒ»éŸ³æ¥½ã®é›°å›²æ°—è¨˜è¿°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

éŸ³æ¥½ã‚¹ã‚¿ã‚¤ãƒ«ä¾‹:
- ambient, cinematic, piano, strings, electronic
- uplifting, contemplative, mysterious, warm, energetic
- documentary style, meditation music, corporate

è©±è€…ã‚¿ã‚¤ãƒ—åˆ¥éŸ³æ¥½ææ¡ˆ:
- ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³(NA)ä¸»ä½“: ã‚¢ãƒ³ãƒ“ã‚¨ãƒ³ãƒˆã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼èª¿ã€ç©ã‚„ã‹ãªæ¥½å™¨
- å¯¾è©±ãƒ»ã‚»ãƒªãƒ•(DL)å¤šã‚: æ„Ÿæƒ…è¡¨ç¾è±Šã‹ã€ãƒ‰ãƒ©ãƒãƒãƒƒã‚¯ã€ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æ€§é‡è¦–
- CPSé«˜é€Ÿéƒ¨åˆ†: ã‚·ãƒ³ãƒ—ãƒ«ãªéŸ³æ¥½ã€é‚ªé­”ã—ãªã„BGM

å¿…ãšJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"""

        # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè§£æçµæœã‚’å«ã‚ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        analysis_info = ""
        if parsed_segments:
            na_count = sum(1 for seg in parsed_segments if getattr(seg, 'speaker_type', '') == 'NARRATOR' or str(getattr(seg, 'speaker_type', '')).endswith('NA'))
            dl_count = sum(1 for seg in parsed_segments if getattr(seg, 'speaker_type', '') == 'DIALOGUE' or str(getattr(seg, 'speaker_type', '')).endswith('DL'))
            total_segments = len(parsed_segments)
            
            analysis_info = f"""

## å°æœ¬è§£æçµæœï¼ˆå‚è€ƒæƒ…å ±ï¼‰:
- ç·ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {total_segments}
- ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³(NA): {na_count}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ ({na_count/total_segments*100:.1f}%)
- å¯¾è©±ãƒ»ã‚»ãƒªãƒ•(DL): {dl_count}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ ({dl_count/total_segments*100:.1f}%)

â€» ã“ã®åˆ†æçµæœã‚’å‚è€ƒã«ã€ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸»ä½“ã‹ã‚»ãƒªãƒ•ä¸»ä½“ã‹ã«å¿œã˜ãŸéŸ³æ¥½æ§‹æˆã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"""

        prompt = f"""ä»¥ä¸‹ã®å°æœ¬ã‹ã‚‰BGMé›°å›²æ°—è¨˜è¿°ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

å°æœ¬:
{script_text}{analysis_info}

è¦æ±‚:
1. å°æœ¬ã®å†…å®¹ãƒ»é›°å›²æ°—ã«åˆã£ãŸBGMã‚­ãƒ¥ãƒ¼ã‚’ææ¡ˆ
2. å„ã‚­ãƒ¥ãƒ¼ã®æ„Ÿæƒ…ãƒ»ãƒ ãƒ¼ãƒ‰ãƒ»BPMãƒ»ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æŒ‡å®š
3. Suno AIç­‰ã®éŸ³æ¥½ç”ŸæˆAIã«é©ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
4. ç´ æã‚µã‚¤ãƒˆæ¤œç´¢ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ

å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰:
{{
  "music_prompts": [
    {{
      "cue_id": "M-001",
      "mood": "warm introspective",
      "bpm": 78,
      "style": "ambient piano with subtle strings",
      "suno_prompt": "Gentle contemplative piano melody with warm ambient textures, soft string pads, 78 BPM, documentary style, meditation mood",
      "stock_keywords": ["contemplative piano", "ambient documentary", "meditation background"]
    }}
  ]
}}"""

        response = self._call_claude_code(prompt, system_prompt)
        if response:
            try:
                # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
                json_start = response.find('{')
                json_end = response.rfind('}') + 1
                if json_start >= 0 and json_end > json_start:
                    json_text = response[json_start:json_end]
                    return json.loads(json_text)
            except json.JSONDecodeError as e:
                print(f"Claude Code JSON parse error: {e}")
                return None
        
        return None

    def generate(self, prompt: str, script_text: str):
        # å®Ÿéš›ã¯OpenAI/Claude APIå‘¼ã³å‡ºã—ã«å·®ã—æ›¿ãˆ
        return {
            "narration": [{"id":"NA-001","text":"äººã¯ãªãœå­¦ã¶ã®ã‹ã€‚"}],
            "dialogues": [{"id":"DL-001","speaker":"äººç‰©A","text":"ãã£ã‹ã‘ã¯å†™çœŸã§ã—ãŸã€‚"}],
            "subtitles": [{"id":"SB-001","text_2line":["äººã¯ãªãœå­¦ã¶ã®ã‹ã€","å¿™ã—ã„æ—¥å¸¸ã®ä¸­ã§ã€‚"]}]
        }
